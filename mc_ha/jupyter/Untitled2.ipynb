{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561fdb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# from ckonlpy.tag import Twitter\n",
    "# from hanspell import spell_checker\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4fd4231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\r\n",
      "------------------ -----------\r\n",
      "aiohttp            3.7.3\r\n",
      "async-generator    1.10\r\n",
      "async-timeout      3.0.1\r\n",
      "attrs              22.1.0\r\n",
      "beautifulsoup4     4.11.1\r\n",
      "brotlipy           0.7.0\r\n",
      "certifi            2022.9.14\r\n",
      "cffi               1.15.1\r\n",
      "chardet            3.0.4\r\n",
      "charset-normalizer 2.0.4\r\n",
      "cryptography       37.0.1\r\n",
      "customized-konlpy  0.0.64\r\n",
      "h11                0.13.0\r\n",
      "idna               3.4\r\n",
      "joblib             1.2.0\r\n",
      "JPype1             1.4.0\r\n",
      "konlpy             0.6.0\r\n",
      "lxml               4.9.1\r\n",
      "multidict          4.7.6\r\n",
      "numpy              1.21.6\r\n",
      "outcome            1.2.0\r\n",
      "pandas             1.3.5\r\n",
      "pip                22.3\r\n",
      "pycparser          2.21\r\n",
      "pyOpenSSL          22.0.0\r\n",
      "PySocks            1.7.1\r\n",
      "python-dateutil    2.8.2\r\n",
      "pytz               2022.5\r\n",
      "requests           2.28.1\r\n",
      "scikit-learn       1.0.2\r\n",
      "scipy              1.7.3\r\n",
      "selenium           3.141.0\r\n",
      "setuptools         63.4.1\r\n",
      "six                1.16.0\r\n",
      "sniffio            1.3.0\r\n",
      "sortedcontainers   2.4.0\r\n",
      "soupsieve          2.3.2.post1\r\n",
      "threadpoolctl      3.1.0\r\n",
      "tqdm               4.64.0\r\n",
      "trio               0.21.0\r\n",
      "trio-websocket     0.9.2\r\n",
      "typing_extensions  4.3.0\r\n",
      "urllib3            1.26.12\r\n",
      "wheel              0.37.1\r\n",
      "wsproto            1.2.0\r\n",
      "yarl               1.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d79d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/naver_movie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f8a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  내가 직접 지정한 불용어 리스트 작성\n",
    "custom_stopwords = [\n",
    "    \"범죄도시\", \"범죄 도시\", \"범죄\", \"도시\", \"죽지\",\n",
    "    \"늑대사냥\", \"늑대\", \"사냥\",\n",
    "    \"공조\", \"인터네셔날\",\"인터내셔날\",\n",
    "    \"한산\", \"용\", \"용의\", \"출현\", \"용의출현\", \"장군님\", \"장군\",\n",
    "    \"뜨거운피\", \"피\", \"뜨거운\",\n",
    "    \"외계인\", \"외계\", \"인\",\n",
    "    \"영화\", \"무비\", \"영화관\", \"한국영화\",\n",
    "    \"진짜\", \"진자\", \"완전\", \"존나\", \"졸라\", \"느낌\", \"뭔가\", \"제일\", \"개\", \"보고\", \"사람\", \"보지\",\n",
    "    \"어요\", \"오늘\", \"그냥\", \"생각\", \"면서\", \"더니\", \"인적\", \"거지\", \"보기\", \"나름\", \"살짝\",\n",
    "    \"정말\", \"대박\", \"역대\", \"최고\", \"어제\", \"편이\", \"계속\", \"요소\", \"처럼\", \"이나\", \"역시\", \"부분\", \"던데\",\n",
    "    \"스포\", \"개봉\", \"한번\", \"내내\", \"구나\", \"때문\", \"어서\", \"정도\", \"다가\", \"다시\", \"누가\", \"덕분\", \"항상\",\n",
    "    \"봤는데\", \"왔는데\",\n",
    "    \"스포일러가 포함된 감상평입니다.\"\n",
    "]\n",
    "\n",
    "#  ckonlpy에 추가할 단어 지정\n",
    "custom_noun = [\n",
    "\n",
    "    #  [ 범죄도시 ]\n",
    "    \"장이수\", \"강해상\", \"손석구\", \"마석도\", \"마동석\", \"마블리\", \"박지환\", \"윤계상\",\n",
    "    \"2부\", \"2탄\", \"2편\", \"1부\", \"1탄\", \"1편\",\n",
    "    #  * 같은 단어 : 장이수=이수, 마석도=마동석=마블리\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 구씨 : 배우 손석구가 다른 작품에서 맡았던 케릭터 이름\n",
    "\n",
    "    #  [ 늑대사냥 ]\n",
    "    \"서인국\", \"장동윤\", \"박호산\", \"정소민\", \"고창석\", \"성동일\",\n",
    "    \"콘에어\", \"콘 에어\", \"잔인\",\n",
    "    #  * 같은 단어 : 콘에어=콘 에어\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 콘에어 : 니콜라스 케이지 출연 1997년 영화. 늑대사냥과 비슷한 소재를 다루고있음.\n",
    "\n",
    "    #  [ 공조2 ]\n",
    "    \"현빈\", \"유해진\", \"윤아\", \"다니엘 헤니\", \"다니엘헤니\", \"다니엘\", \"헤니\", \"진선규\",\n",
    "    \"장영남\", \"박훈\", \"임성재\", \"림청렬\", \"청렬\", \"강진태\", \"진태\", \"장명준\", \"박소연\",\n",
    "    \"박상위\", \"박민영\",\n",
    "    \"불시착\", \"사랑의 불시착\",\n",
    "    # 같은 단어 : 다니엘헤니=다니엘=헤니, 불시착=사랑의 불시착\n",
    "\n",
    "    #  [ 외계인1부 ]\n",
    "    \"류준열\", \"김우빈\", \"김태리\", \"소지섭\", \"염정아\", \"조우진\", \"이하늬\",\n",
    "    \"무륵\", \"김현중\", \"이안\", \"문도석\", \"흑설\", \"청운\",\n",
    "    \"2부\", \"2탄\", \"2편\", \"1부\", \"1탄\", \"1편\", \"돈\", \"SF\", \"sf\", \"SF물\", \"sf물\",\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 돈 : 한 글자 '돈 아깝다'같은 말이 종종 보임.\n",
    "    #  - SF, sf : 장르명. 한글이 아니라고해서 없애면 안됨.\n",
    "\n",
    "    #  [ 한산 ]\n",
    "    \"박해일\", \"안성기\", \"변요한\", \"손현주\", \"김성규\", \"김성균\", \"김향기\", \"택연\", \"옥택연\",\n",
    "    \"이순신\", \"장군님\", \"장군\", \"어영담\", \"원균\", \"준산\", \"히데요시\", \"도요토미\", \"도요토미 히데요시\",\n",
    "    \"한산도\", \"대첩\", \"한산도 대첩\", \"임진왜란\", \"유키나가\", \"학익진\",\n",
    "    #  * 같은 단어 : 이순신=이순신장군\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 하라 : '전군 출정하라', '선회하라', '발포하라'가 이 영화의 명대사인가봄.\n",
    "\n",
    "    # 뜨거운피 배우명/등장인물명\n",
    "    \"정우\", \"김갑수\", \"최무성\", \"지승현\", \"김해곤\", \"윤지혜\", \"이홍내\", \"정호빈\"\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 짱구 : 배우 정우가 다른 작품에서 맡았던 케릭터 이름\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e0d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 전처리 모듈\n",
    "def preproc(reviews, custom_stopwords, custom_noun, spellcheck=False) :\n",
    "    print(\"전처리 시작\")\n",
    "    #  문서군 변수 생성\n",
    "    documents = []\n",
    "\n",
    "    #  기존 불용어 위에 사용자 지정 불용어 적용\n",
    "    with open(\"./data/한국어불용어_new.txt\", \"r\", encoding=\"utf-8\") as f :\n",
    "        stop_words = f.read()\n",
    "    f.close()\n",
    "    stop_words = stop_words.split(\",\")\n",
    "    stop_words.extend(custom_stopwords)\n",
    "\n",
    "    #  사용자 지정 단어 사전 추가\n",
    "    custom_okt = Twitter()\n",
    "    for n in custom_noun:\n",
    "        custom_okt.add_dictionary(n, \"Noun\")\n",
    "\n",
    "    #  결측치가 존재하는 record 제거\n",
    "    reviews.dropna(inplace=True)\n",
    "\n",
    "    #  의미있는한글(ㅋㅋ, ㅎㅎ이런거 말고), 알파벳, 숫자, 띄어쓰기 제외한 글자 삭제\n",
    "    for idx, comment in enumerate(reviews[\"comment\"]) :\n",
    "        comment = re.sub(r\"[^가-힣a-zA-Z0-9 ]\", \"\", comment)\n",
    "        reviews.iloc[idx,2] = comment\n",
    "\n",
    "    #  띄어쓰기, 맞춤법 자동 교정 (주의!! 엄청 오래걸림!!!)\n",
    "    if spellcheck == True :\n",
    "        for comment in tqdm(reviews[\"comment\"]) :\n",
    "            comment = spell_checker.check(comment).checked\n",
    "            reviews.iloc[idx,2] = comment\n",
    "\n",
    "    #  불용어 제거, 한글자 단어 제거\n",
    "    for comment in reviews[\"comment\"] :\n",
    "        document = \"\"\n",
    "        words = custom_okt.nouns(comment)\n",
    "        for word in words :\n",
    "            if (len(word) >= 2) or (word in custom_noun) :\n",
    "                if word not in stop_words :\n",
    "                    document += word + \" \"\n",
    "        document = document.rstrip()\n",
    "        documents.append(document)\n",
    "\n",
    "    print(\"전처리 완료, 문서군 생성\\n\")\n",
    "    #  문서군 변수 리턴\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ec77c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt(documents, filename=\"\", cnt_th=10) :\n",
    "    print(\"빈도수 기반 키워드 추출 시작\")\n",
    "    #  문서군 내의 모든 문서들을 하나의 문서로 모음\n",
    "    documents_sum = []\n",
    "    for document in documents :\n",
    "        words = document.split(\" \")\n",
    "        if words[0] != \"\" :   # 공백이 카운트되는것 방지용\n",
    "            documents_sum.append(words)\n",
    "    documents_sum = sum(documents_sum, [])\n",
    "\n",
    "    #  단어들의 빈도수 세기\n",
    "    counter = Counter(documents_sum)\n",
    "    vocab_sorted = sorted(counter.items(),  key=lambda x : x[1], reverse=True)\n",
    "\n",
    "    #  빈도수 cnt_th 이상인 단어만 추출\n",
    "    vocab_result = []\n",
    "    for vocab in vocab_sorted :\n",
    "        if vocab[1] >= cnt_th :\n",
    "            vocab_result.append(vocab)\n",
    "\n",
    "    #  변수 vocab_result를 데이터프레임으로 만들기\n",
    "    data = np.array([None for _ in range(len(vocab_result)*2)])\n",
    "    data = data.reshape((len(vocab_result),2))\n",
    "    result_df = pd.DataFrame(data, columns=[\"noun\", \"count\"])\n",
    "\n",
    "    for idx, vocab in enumerate(vocab_result) :\n",
    "        result_df.iloc[idx, 0] = vocab[0]\n",
    "        result_df.iloc[idx, 1] = vocab[1]\n",
    "\n",
    "    #  지정한 파일명으로 csv 파일 저장\n",
    "    #  (파일명을 지정하지 않으면 파일로 저장되지 않고 리턴만 함.)\n",
    "    if filename != \"\" :\n",
    "        result_df.to_csv(\"{}.csv\".format(filename), index=False, sep=\",\", encoding=\"utf-8\")\n",
    "        print(\"csv파일 저장 완료\")\n",
    "\n",
    "    print(\"빈도수 기반 키워드 추출 완료\\n\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f656eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 유튜브 댓글\n",
    "file_names = glob(\"./result/크롤링_유튜브댓글/*.csv\")\n",
    "\n",
    "for file_name in file_names :\n",
    "    reviews = pd.read_csv(file_name, encoding='utf-8', sep=\",\")\n",
    "    documents = preproc(reviews, custom_stopwords, custom_noun, spellcheck=False)\n",
    "    file_name = re.sub(r\".*\\\\\", \"\", file_name)  \n",
    "    file_name = re.sub(r\"\\.csv\", \"\", file_name)  # 파일 저장하기 편하려고...\n",
    "    \n",
    "    #  빈도수 기반 키워드 추출\n",
    "    if file_name == \"뜨거운피_예고편\" :\n",
    "        cnt_th = 4  # 빈도수 4 이상만 추출\n",
    "    else :\n",
    "        cnt_th = 10  # 빈도수 10 이상만 추출\n",
    "    result_cnt = cnt(documents, filename=f\"단어빈도수_유튜브_{file_name}\", cnt_th=cnt_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "219b1774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.isna()['장르'][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f86e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.read_csv('./카테고리 별 해당 키워드.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288d3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = cat.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19c13aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat['기타'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0f6e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = ['마동석', '배우', '쓰레기', '맥북']\n",
    "\n",
    "cat_list = []\n",
    "for c in cat.columns:\n",
    "    for i in range(len(cat)):\n",
    "        if cat.isna()[c][i] != True:\n",
    "            if cat[c][i] not in cat_list:\n",
    "                cat_list.append(cat[c][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44378ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0\n",
    "for t in t_list:\n",
    "    if t in cat_list:\n",
    "        w += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5efb40e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b00ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140426bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8365e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f457426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a1079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc",
   "language": "python",
   "name": "mc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
